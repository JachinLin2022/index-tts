{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef59f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:50:34 [__init__.py:243] Automatically detected platform cuda.\n",
      "✅  Registry GPT2TTSModel to vllm\n",
      "✅  SamplingParams._verify_args Patched\n",
      "✅  ModelInputForGPUBuilder._compute_lens Patched\n",
      ">> All random seeds have been set to 42 for reproducibility.\n",
      "INFO 09-16 14:50:50 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 09-16 14:50:50 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 09-16 14:50:50 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 09-16 14:50:50 [config.py:3131] Downcasting torch.float32 to torch.float16.\n",
      "INFO 09-16 14:50:50 [config.py:793] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-16 14:50:50 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0) with config: model='checkpoints/vllm', speculative_config=None, tokenizer='checkpoints/vllm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1818, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=checkpoints/vllm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \n",
      "INFO 09-16 14:50:50 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 09-16 14:50:51 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-16 14:50:51 [model_runner.py:1170] Starting to load model checkpoints/vllm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d403d418f41ccb00c5aa36e596005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:50:54 [default_loader.py:280] Loading weights took 3.48 seconds\n",
      "INFO 09-16 14:50:55 [model_runner.py:1202] Model loading took 0.9242 GiB and 3.522130 seconds\n",
      "WARNING 09-16 14:50:55 [profiling.py:72] `get_dummy_processor_inputs` has been split up into `get_dummy_text` and `get_dummy_mm_data`. These two methods will be marked as abstract in an upcoming release.\n",
      "INFO 09-16 14:50:56 [worker.py:291] Memory profiling takes 0.91 seconds\n",
      "INFO 09-16 14:50:56 [worker.py:291] the current vLLM instance can use total_gpu_memory (23.53GiB) x gpu_memory_utilization (0.50) = 11.76GiB\n",
      "INFO 09-16 14:50:56 [worker.py:291] model weights take 0.92GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 0.17GiB; the rest of the memory reserved for KV Cache is 10.66GiB.\n",
      "INFO 09-16 14:50:56 [executor_base.py:112] # cuda blocks: 5819, # CPU blocks: 2184\n",
      "INFO 09-16 14:50:56 [executor_base.py:117] Maximum concurrency for 1818 tokens per request: 51.21x\n",
      "INFO 09-16 14:51:02 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5090115fde4aaf92de0379966f5f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:51:27 [model_runner.py:1670] Graph capturing finished in 25 secs, took 0.23 GiB\n",
      "INFO 09-16 14:51:27 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 32.09 seconds\n",
      ">> GPT weights restored from: checkpoints/gpt.pth\n",
      ">> semantic_codec weights restored from: ./checkpoints/hf_cache/models--amphion--MaskGCT/snapshots/265c6cef07625665d0c28d2faafb1415562379dc/semantic_codec/model.safetensors\n",
      "cfm loaded\n",
      "length_regulator loaded\n",
      "gpt_layer loaded\n",
      ">> s2mel weights restored from: checkpoints/s2mel.pth\n",
      ">> campplus_model weights restored from: ./checkpoints/hf_cache/models--funasr--campplus/snapshots/fb71fe990cbf6031ae6987a2d76fe64f94377b7e/campplus_cn_common.bin\n",
      "Loading weights from nvidia/bigvgan_v2_22khz_80band_256x\n",
      "Removing weight norm...\n",
      ">> bigvgan weights restored from: nvidia/bigvgan_v2_22khz_80band_256x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:51:48,105 WETEXT INFO found existing fst: /workspace/RichooAgent/third_party/index-tts/indextts/utils/tagger_cache/zh_tn_tagger.fst\n",
      "2025-09-16 14:51:48,107 WETEXT INFO                     /workspace/RichooAgent/third_party/index-tts/indextts/utils/tagger_cache/zh_tn_verbalizer.fst\n",
      "2025-09-16 14:51:48,107 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2025-09-16 14:51:48,612 WETEXT INFO found existing fst: /venv/main/lib/python3.12/site-packages/tn/en_tn_tagger.fst\n",
      "2025-09-16 14:51:48,613 WETEXT INFO                     /venv/main/lib/python3.12/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-09-16 14:51:48,614 WETEXT INFO skip building fst for en_normalizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> TextNormalizer loaded\n",
      ">> bpe model loaded from: checkpoints/bpe.model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "from indextts.infer_v2_vllm import IndexTTS2\n",
    "# from indextts.infer_v2 import IndexTTS2\n",
    "import asyncio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index_tts = IndexTTS2(cfg_path=\"checkpoints/config.yaml\", model_dir=\"checkpoints\", use_cuda_kernel=False, gpu_memory_utilization=0.5)\n",
    "# index_tts = IndexTTS2(cfg_path=\"checkpoints/config.yaml\", model_dir=\"checkpoints\", use_cuda_kernel=False)\n",
    "# await index_tts.infer(spk_audio_prompt='data/test_real.wav', text=\"Mason has been working on his game, and a key area we've identified for him to focus on is his smash technique.\\n\\nDeveloping a strong smash is crucial for an attacking game in badminton. Mason should focus on getting a high point of contact and a powerful wrist snap to generate more speed and angle on the shuttle. He could try shadow drills at home, mimicking the full smash motion, paying close attention to the wrist action. Consistent practice will help him build muscle memory and increase the power and accuracy of his attacking shots, making him a more formidable player on the court.\", output_path=\"test.wav\", verbose=False, max_text_tokens_per_segment=1200)\n",
    "\n",
    "\n",
    "\n",
    "# await index_tts.infer_fast(spk_audio_prompt='data/tina_test.wav', text=\"Mike is showing great potential in his Fall In-Person Chess Lessons. We've observed that chess is a wonderful activity for him, significantly helping to build his focus and concentration. To further support his development, we believe he can benefit from exploring additional resources. To help Mike continue advancing, we recommend a two-pronged approach. First, to deepen his understanding and tactical skills, Mike could explore beginner-friendly chess books like Chess for Children or Bobby Fischer Teaches Chess. These resources make learning fun and can significantly improve his strategic thinking. Second, engaging in online puzzles and supervised tournaments on platforms like ChessKid will provide valuable practice and expose him to different playing styles in a supportive environment. This consistent engagement will not only enhance his chess level but also reinforce his focus and problem-solving abilities.\", output_path=\"test2_no_emo.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0)\n",
    "\n",
    "# await index_tts.infer_parallel(spk_audio_prompt='data/tina_test.wav', text=\"\"\"Mike is showing great potential in his Fall In-Person Chess Lessons.\"\"\", output_path=\"test_parallel.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43dfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "async def infer_parallel(self, spk_audio_prompt, text, output_path,\n",
    "                       emo_audio_prompt=None, emo_alpha=1.0,\n",
    "                       emo_vector=None,\n",
    "                       use_emo_text=False, emo_text=None, use_random=False, interval_silence=200,\n",
    "                       verbose=False, max_text_tokens_per_segment=120, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    [优化版本] 使用 VLLM 并行处理所有文本片段的推理。\n",
    "    \n",
    "    此函数首先将长文本分割成多个片段，然后将所有片段的生成请求\n",
    "    一次性、并发地提交给VLLM引擎。在收到所有代码序列后，\n",
    "    再串行地通过后续的声码器模型生成音频，最后拼接成完整的语音。\n",
    "    这大大减少了自回归生成所花费的总时间。\n",
    "    \"\"\"\n",
    "    print(\">> Starting parallel inference...\")\n",
    "    # self._set_gr_progress(0, \"start inference...\") # 如果在Gradio等UI环境，可以取消注释\n",
    "    if verbose:\n",
    "        print(f\"Original text: {text}, spk_audio_prompt: {spk_audio_prompt}, \"\n",
    "            f\"emo_audio_prompt: {emo_audio_prompt}, emo_alpha: {emo_alpha}, \"\n",
    "            f\"emo_vector: {emo_vector}, use_emo_text: {use_emo_text}, \"\n",
    "            f\"emo_text: {emo_text}\")\n",
    "    start_time = time.perf_counter()\n",
    "    # --- 1. 初始化和条件准备 (与原始代码相同) ---\n",
    "    # 这部分代码只执行一次，用于准备所有片段共享的条件向量\n",
    "    if use_emo_text:\n",
    "        emo_audio_prompt = None\n",
    "        emo_alpha = 1.0\n",
    "        if emo_text is None:\n",
    "            emo_text = text\n",
    "        emo_dict = self.qwen_emo.inference(emo_text)\n",
    "        print(emo_dict)\n",
    "        emo_vector = list(emo_dict.values())\n",
    "    if emo_vector is not None:\n",
    "        emo_audio_prompt = None\n",
    "        emo_alpha = 1.0\n",
    "    if emo_audio_prompt is None:\n",
    "        emo_audio_prompt = spk_audio_prompt\n",
    "        emo_alpha = 1.0\n",
    "    if self.cache_spk_cond is None or self.cache_spk_audio_prompt != spk_audio_prompt:\n",
    "        audio, sr = librosa.load(spk_audio_prompt)\n",
    "        audio = torch.tensor(audio).unsqueeze(0)\n",
    "        audio_22k = torchaudio.transforms.Resample(sr, 22050)(audio)\n",
    "        audio_16k = torchaudio.transforms.Resample(sr, 16000)(audio)\n",
    "        inputs = self.extract_features(audio_16k, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        input_features = inputs[\"input_features\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "        spk_cond_emb = self.get_emb(input_features, attention_mask)\n",
    "        _, S_ref = self.semantic_codec.quantize(spk_cond_emb)\n",
    "        ref_mel = self.mel_fn(audio_22k.to(spk_cond_emb.device).float())\n",
    "        ref_target_lengths = torch.LongTensor([ref_mel.size(2)]).to(ref_mel.device)\n",
    "        feat = torchaudio.compliance.kaldi.fbank(audio_16k.to(ref_mel.device), num_mel_bins=80, dither=0, sample_frequency=16000)\n",
    "        feat = feat - feat.mean(dim=0, keepdim=True)\n",
    "        style = self.campplus_model(feat.unsqueeze(0))\n",
    "        prompt_condition = self.s2mel.models['length_regulator'](S_ref, ylens=ref_target_lengths, n_quantizers=3, f0=None)[0]\n",
    "        self.cache_spk_cond = spk_cond_emb\n",
    "        self.cache_s2mel_style = style\n",
    "        self.cache_s2mel_prompt = prompt_condition\n",
    "        self.cache_spk_audio_prompt = spk_audio_prompt\n",
    "        self.cache_mel = ref_mel\n",
    "    else:\n",
    "        style = self.cache_s2mel_style\n",
    "        prompt_condition = self.cache_s2mel_prompt\n",
    "        spk_cond_emb = self.cache_spk_cond\n",
    "        ref_mel = self.cache_mel\n",
    "    if emo_vector is not None:\n",
    "        weight_vector = torch.tensor(emo_vector).to(self.device)\n",
    "        if use_random:\n",
    "            random_index = [random.randint(0, x - 1) for x in self.emo_num]\n",
    "        else:\n",
    "            random_index = [find_most_similar_cosine(style, tmp) for tmp in self.spk_matrix]\n",
    "        emo_matrix = [tmp[index].unsqueeze(0) for index, tmp in zip(random_index, self.emo_matrix)]\n",
    "        emo_matrix = torch.cat(emo_matrix, 0)\n",
    "        emovec_mat = weight_vector.unsqueeze(1) * emo_matrix\n",
    "        emovec_mat = torch.sum(emovec_mat, 0).unsqueeze(0)\n",
    "    if self.cache_emo_cond is None or self.cache_emo_audio_prompt != emo_audio_prompt:\n",
    "        emo_audio, _ = librosa.load(emo_audio_prompt, sr=16000)\n",
    "        emo_inputs = self.extract_features(emo_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        emo_input_features = emo_inputs[\"input_features\"].to(self.device)\n",
    "        emo_attention_mask = emo_inputs[\"attention_mask\"].to(self.device)\n",
    "        emo_cond_emb = self.get_emb(emo_input_features, emo_attention_mask)\n",
    "        self.cache_emo_cond = emo_cond_emb\n",
    "        self.cache_emo_audio_prompt = emo_audio_prompt\n",
    "    else:\n",
    "        emo_cond_emb = self.cache_emo_cond\n",
    "        \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', enabled=self.dtype is not None, dtype=self.dtype):\n",
    "        emovec = self.gpt.merge_emovec(\n",
    "            spk_cond_emb,\n",
    "            emo_cond_emb,\n",
    "            torch.tensor([spk_cond_emb.shape[-1]], device=self.device),\n",
    "            torch.tensor([emo_cond_emb.shape[-1]], device=self.device),\n",
    "            alpha=emo_alpha\n",
    "        )\n",
    "        if emo_vector is not None:\n",
    "            emovec = emovec_mat + (1 - torch.sum(weight_vector)) * emovec\n",
    "        \n",
    "    # --- 2. 文本分段和批处理准备 ---\n",
    "    # self._set_gr_progress(0.1, \"text processing...\")\n",
    "    text_tokens_list = self.tokenizer.tokenize(text)\n",
    "    segments = self.tokenizer.split_segments(text_tokens_list, max_text_tokens_per_segment)\n",
    "    num_segments = len(segments)\n",
    "    if num_segments == 0:\n",
    "        print(\">> No text segments to process.\")\n",
    "        return None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Text split into {num_segments} segments.\")\n",
    "        print(*segments, sep=\"\\n\")\n",
    "    \n",
    "    text_tokens_batch = [\n",
    "        torch.tensor(self.tokenizer.convert_tokens_to_ids(sent), dtype=torch.int32, device=self.device)\n",
    "        for sent in segments\n",
    "    ]\n",
    "    # --- 3. 并发执行 VLLM 推理 ---\n",
    "    gpt_gen_time_start = time.perf_counter()\n",
    "    \n",
    "    # 扩展条件张量以匹配批次大小\n",
    "    batched_spk_cond_emb = spk_cond_emb.repeat(num_segments, 1, 1)\n",
    "    batched_emo_cond_emb = emo_cond_emb.repeat(num_segments, 1, 1)\n",
    "    batched_emovec = emovec.repeat(num_segments, 1, 1)\n",
    "    batched_cond_lengths = torch.tensor([spk_cond_emb.shape[-1]], device=self.device).repeat(num_segments)\n",
    "    batched_emo_cond_lengths = torch.tensor([emo_cond_emb.shape[-1]], device=self.device).repeat(num_segments)\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', enabled=self.dtype is not None, dtype=self.dtype):\n",
    "        all_results, all_latents = await self.gpt.inference_speech_vllm(\n",
    "            batched_spk_cond_emb,\n",
    "            text_tokens_batch,\n",
    "            batched_emo_cond_emb,\n",
    "            cond_lengths=batched_cond_lengths,\n",
    "            emo_cond_lengths=batched_emo_cond_lengths,\n",
    "            emo_vec=batched_emovec,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    gpt_gen_time = time.perf_counter() - gpt_gen_time_start\n",
    "    # --- 4. 串行处理后续步骤 (声码器等) ---\n",
    "    wavs = []\n",
    "    gpt_forward_time = 0\n",
    "    s2mel_time = 0\n",
    "    bigvgan_time = 0\n",
    "    has_warned = False\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        codes = torch.tensor(all_results[i], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        speech_conditioning_latent = all_latents[i:i+1]\n",
    "        # unsqueeze a batch dim for text tokens, because old code expected a list of tensors with batch dim\n",
    "        text_tokens = text_tokens_batch[i].unsqueeze(0) \n",
    "        if not has_warned and (codes[:, -1] != self.stop_mel_token).any():\n",
    "            max_mel_tokens = generation_kwargs.get(\"max_mel_tokens\", 1500)\n",
    "            warnings.warn(\n",
    "                f\"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). \"\n",
    "                f\"Input text tokens: {text_tokens.shape[1]}. \"\n",
    "                f\"Consider reducing `max_text_tokens_per_segment`({max_text_tokens_per_segment}) or increasing `max_mel_tokens`.\",\n",
    "                category=RuntimeWarning\n",
    "            )\n",
    "            has_warned = True\n",
    "        \n",
    "        code_len = codes.shape[-1]\n",
    "        if self.stop_mel_token in codes[0]:\n",
    "            stop_indices = (codes[0] == self.stop_mel_token).nonzero(as_tuple=False)\n",
    "            if len(stop_indices) > 0:\n",
    "                code_len = stop_indices[0, 0].item()\n",
    "        codes = codes[:, :code_len]\n",
    "        code_lens = torch.tensor([code_len], dtype=torch.long, device=self.device)\n",
    "        if verbose:\n",
    "            print(f\"Segment {i+1}/{num_segments}, generated codes shape: {codes.shape}\")\n",
    "        m_start_time = time.perf_counter()\n",
    "        use_speed = torch.zeros(1, device=self.device).long()\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=self.dtype is not None, dtype=self.dtype):\n",
    "            # ================================== FIX START ==================================\n",
    "            # 使用从批处理张量中切片出的、与当前片段对应的条件向量，\n",
    "            # 确保与 VLLM 生成 speech_conditioning_latent 时使用的上下文完全一致。\n",
    "            current_emo_cond_emb = batched_emo_cond_emb[i:i+1]\n",
    "            current_emovec = batched_emovec[i:i+1]\n",
    "            \n",
    "            latent = self.gpt(\n",
    "                speech_conditioning_latent,\n",
    "                text_tokens,\n",
    "                torch.tensor([text_tokens.shape[-1]], device=self.device),\n",
    "                codes,\n",
    "                code_lens, # 使用修正后的code_lens\n",
    "                current_emo_cond_emb,\n",
    "                cond_mel_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=self.device),\n",
    "                emo_cond_mel_lengths=torch.tensor([current_emo_cond_emb.shape[1]], device=self.device),\n",
    "                emo_vec=current_emovec.squeeze(1),\n",
    "                use_speed=use_speed,\n",
    "            )\n",
    "            # =================================== FIX END ===================================\n",
    "        gpt_forward_time += time.perf_counter() - m_start_time\n",
    "        dtype = None\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=dtype is not None, dtype=dtype):\n",
    "            m_start_time = time.perf_counter()\n",
    "            latent = self.s2mel.models['gpt_layer'](latent)\n",
    "            S_infer = self.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1)).transpose(1, 2)\n",
    "            S_infer = S_infer + latent\n",
    "            target_lengths = (code_lens * 1.72).long()\n",
    "            cond = self.s2mel.models['length_regulator'](S_infer, ylens=target_lengths, n_quantizers=3, f0=None)[0]\n",
    "            \n",
    "            cat_condition = torch.cat([prompt_condition, cond], dim=1)\n",
    "            diffusion_steps = 13\n",
    "            inference_cfg_rate = 0.7\n",
    "            vc_target = self.s2mel.models['cfm'].inference(\n",
    "                cat_condition,\n",
    "                torch.LongTensor([cat_condition.size(1)]).to(cond.device),\n",
    "                ref_mel, style, None, \n",
    "                diffusion_steps,\n",
    "                inference_cfg_rate=inference_cfg_rate\n",
    "            )\n",
    "            vc_target = vc_target[:, :, ref_mel.size(-1):]\n",
    "            s2mel_time += time.perf_counter() - m_start_time\n",
    "            m_start_time = time.perf_counter()\n",
    "            wav = self.bigvgan(vc_target.float()).squeeze(1)\n",
    "            bigvgan_time += time.perf_counter() - m_start_time\n",
    "        wav = torch.clamp(32767 * wav, -32767.0, 32767.0)\n",
    "        wavs.append(wav.cpu())\n",
    "    # --- 5. 合成并保存音频 ---\n",
    "    end_time = time.perf_counter()\n",
    "    sampling_rate = 22050\n",
    "    \n",
    "    # ======================= AUDIO STITCHING IMPROVEMENT =======================\n",
    "    # 推荐使用交叉淡化以获得最平滑的拼接效果\n",
    "    crossfade_ms = 150 # 较短的交叉淡化时长，避免模糊发音\n",
    "    wav = self.crossfade_torch(wavs, crossfade_duration_ms=crossfade_ms, sampling_rate=sampling_rate)\n",
    "    # 如果您仍然倾向于使用静音间隔，可以取消注释下面的代码块\n",
    "    # if interval_silence > 0 and len(wavs) > 1:\n",
    "    #     wavs = self.insert_interval_silence(wavs, sampling_rate=sampling_rate, interval_silence=interval_silence)\n",
    "    # wav = torch.cat(wavs, dim=1) if wavs else torch.tensor([])\n",
    "    # ===========================================================================\n",
    "    if wav.numel() == 0:\n",
    "        print(\">> No audio was generated.\")\n",
    "        return None\n",
    "    wav_length = wav.shape[-1] / sampling_rate\n",
    "    print(f\">> GPT generation time (parallel): {gpt_gen_time:.2f} seconds\")\n",
    "    print(f\">> GPT forward pass time: {gpt_forward_time:.2f} seconds\")\n",
    "    print(f\">> S2MEL time: {s2mel_time:.2f} seconds\")\n",
    "    print(f\">> BigVGAN time: {bigvgan_time:.2f} seconds\")\n",
    "    print(f\">> Total inference time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\">> Generated audio length: {wav_length:.2f} seconds\")\n",
    "    print(f\">> Real-Time Factor (RTF): {(end_time - start_time) / wav_length:.4f}\")\n",
    "    wav = wav.cpu()\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        torchaudio.save(output_path, wav.to(torch.int16), sampling_rate)\n",
    "        print(f\">> WAV file saved to: {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        wav_data = wav.to(torch.int16).numpy().T\n",
    "        return (sampling_rate, wav_data)\n",
    "# index_tts.infer_parallel = infer_parallel\n",
    "import types\n",
    "index_tts.infer_parallel = types.MethodType(infer_parallel, index_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd48515",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def infer(self, spk_audio_prompt, text, output_path,\n",
    "              emo_audio_prompt=None, emo_alpha=1.0,\n",
    "              emo_vector=None,\n",
    "              use_emo_text=False, emo_text=None, use_random=False, interval_silence=200,\n",
    "              verbose=False, max_text_tokens_per_segment=120, **generation_kwargs):\n",
    "        print(\">> start inference...\")\n",
    "        self._set_gr_progress(0, \"start inference...\")\n",
    "        if verbose:\n",
    "            print(f\"origin text:{text}, spk_audio_prompt:{spk_audio_prompt},\"\n",
    "                  f\" emo_audio_prompt:{emo_audio_prompt}, emo_alpha:{emo_alpha}, \"\n",
    "                  f\"emo_vector:{emo_vector}, use_emo_text:{use_emo_text}, \"\n",
    "                  f\"emo_text:{emo_text}\")\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        if use_emo_text:\n",
    "            emo_audio_prompt = None\n",
    "            emo_alpha = 1.0\n",
    "            # assert emo_audio_prompt is None\n",
    "            # assert emo_alpha == 1.0\n",
    "            if emo_text is None:\n",
    "                emo_text = text\n",
    "            emo_dict = self.qwen_emo.inference(emo_text)\n",
    "            print(emo_dict)\n",
    "            # convert ordered dict to list of vectors; the order is VERY important!\n",
    "            emo_vector = list(emo_dict.values())\n",
    "\n",
    "        if emo_vector is not None:\n",
    "            emo_audio_prompt = None\n",
    "            emo_alpha = 1.0\n",
    "            # assert emo_audio_prompt is None\n",
    "            # assert emo_alpha == 1.0\n",
    "\n",
    "        if emo_audio_prompt is None:\n",
    "            emo_audio_prompt = spk_audio_prompt\n",
    "            emo_alpha = 1.0\n",
    "            # assert emo_alpha == 1.0\n",
    "\n",
    "        # 如果参考音频改变了，才需要重新生成, 提升速度\n",
    "        if self.cache_spk_cond is None or self.cache_spk_audio_prompt != spk_audio_prompt:\n",
    "            audio, sr = librosa.load(spk_audio_prompt)\n",
    "            audio = torch.tensor(audio).unsqueeze(0)\n",
    "            audio_22k = torchaudio.transforms.Resample(sr, 22050)(audio)\n",
    "            audio_16k = torchaudio.transforms.Resample(sr, 16000)(audio)\n",
    "\n",
    "            inputs = self.extract_features(audio_16k, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            input_features = inputs[\"input_features\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            input_features = input_features.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            spk_cond_emb = self.get_emb(input_features, attention_mask)\n",
    "\n",
    "            _, S_ref = self.semantic_codec.quantize(spk_cond_emb)\n",
    "            ref_mel = self.mel_fn(audio_22k.to(spk_cond_emb.device).float())\n",
    "            ref_target_lengths = torch.LongTensor([ref_mel.size(2)]).to(ref_mel.device)\n",
    "            feat = torchaudio.compliance.kaldi.fbank(audio_16k.to(ref_mel.device),\n",
    "                                                     num_mel_bins=80,\n",
    "                                                     dither=0,\n",
    "                                                     sample_frequency=16000)\n",
    "            feat = feat - feat.mean(dim=0, keepdim=True)  # feat2另外一个滤波器能量组特征[922, 80]\n",
    "            style = self.campplus_model(feat.unsqueeze(0))  # 参考音频的全局style2[1,192]\n",
    "\n",
    "            prompt_condition = self.s2mel.models['length_regulator'](S_ref,\n",
    "                                                                     ylens=ref_target_lengths,\n",
    "                                                                     n_quantizers=3,\n",
    "                                                                     f0=None)[0]\n",
    "\n",
    "            self.cache_spk_cond = spk_cond_emb\n",
    "            self.cache_s2mel_style = style\n",
    "            self.cache_s2mel_prompt = prompt_condition\n",
    "            self.cache_spk_audio_prompt = spk_audio_prompt\n",
    "            self.cache_mel = ref_mel\n",
    "        else:\n",
    "            style = self.cache_s2mel_style\n",
    "            prompt_condition = self.cache_s2mel_prompt\n",
    "            spk_cond_emb = self.cache_spk_cond\n",
    "            ref_mel = self.cache_mel\n",
    "\n",
    "        if emo_vector is not None:\n",
    "            weight_vector = torch.tensor(emo_vector).to(self.device)\n",
    "            if use_random:\n",
    "                random_index = [random.randint(0, x - 1) for x in self.emo_num]\n",
    "            else:\n",
    "                random_index = [find_most_similar_cosine(style, tmp) for tmp in self.spk_matrix]\n",
    "\n",
    "            emo_matrix = [tmp[index].unsqueeze(0) for index, tmp in zip(random_index, self.emo_matrix)]\n",
    "            emo_matrix = torch.cat(emo_matrix, 0)\n",
    "            emovec_mat = weight_vector.unsqueeze(1) * emo_matrix\n",
    "            emovec_mat = torch.sum(emovec_mat, 0)\n",
    "            emovec_mat = emovec_mat.unsqueeze(0)\n",
    "\n",
    "        if self.cache_emo_cond is None or self.cache_emo_audio_prompt != emo_audio_prompt:\n",
    "            emo_audio, _ = librosa.load(emo_audio_prompt, sr=16000)\n",
    "            emo_inputs = self.extract_features(emo_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            emo_input_features = emo_inputs[\"input_features\"]\n",
    "            emo_attention_mask = emo_inputs[\"attention_mask\"]\n",
    "            emo_input_features = emo_input_features.to(self.device)\n",
    "            emo_attention_mask = emo_attention_mask.to(self.device)\n",
    "            emo_cond_emb = self.get_emb(emo_input_features, emo_attention_mask)\n",
    "\n",
    "            self.cache_emo_cond = emo_cond_emb\n",
    "            self.cache_emo_audio_prompt = emo_audio_prompt\n",
    "        else:\n",
    "            emo_cond_emb = self.cache_emo_cond\n",
    "\n",
    "        self._set_gr_progress(0.1, \"text processing...\")\n",
    "        text_tokens_list = self.tokenizer.tokenize(text)\n",
    "        segments = self.tokenizer.split_segments(text_tokens_list, max_text_tokens_per_segment)\n",
    "        if verbose:\n",
    "            print(\"text_tokens_list:\", text_tokens_list)\n",
    "            print(\"segments count:\", len(segments))\n",
    "            print(\"max_text_tokens_per_segment:\", max_text_tokens_per_segment)\n",
    "            print(*segments, sep=\"\\n\")\n",
    "        do_sample = generation_kwargs.pop(\"do_sample\", True)\n",
    "        top_p = generation_kwargs.pop(\"top_p\", 0.8)\n",
    "        top_k = generation_kwargs.pop(\"top_k\", 30)\n",
    "        temperature = generation_kwargs.pop(\"temperature\", 0.8)\n",
    "        autoregressive_batch_size = 1\n",
    "        length_penalty = generation_kwargs.pop(\"length_penalty\", 0.0)\n",
    "        num_beams = generation_kwargs.pop(\"num_beams\", 3)\n",
    "        repetition_penalty = generation_kwargs.pop(\"repetition_penalty\", 10.0)\n",
    "        max_mel_tokens = generation_kwargs.pop(\"max_mel_tokens\", 1500)\n",
    "        sampling_rate = 22050\n",
    "\n",
    "        wavs = []\n",
    "        gpt_gen_time = 0\n",
    "        gpt_forward_time = 0\n",
    "        s2mel_time = 0\n",
    "        bigvgan_time = 0\n",
    "        progress = 0\n",
    "        has_warned = False\n",
    "        for sent in segments:\n",
    "            text_tokens = self.tokenizer.convert_tokens_to_ids(sent)\n",
    "            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)\n",
    "            if verbose:\n",
    "                print(text_tokens)\n",
    "                print(f\"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}\")\n",
    "                # debug tokenizer\n",
    "                text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())\n",
    "                print(\"text_token_syms is same as segment tokens\", text_token_syms == sent)\n",
    "\n",
    "            m_start_time = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):\n",
    "                    emovec = self.gpt.merge_emovec(\n",
    "                        spk_cond_emb,\n",
    "                        emo_cond_emb,\n",
    "                        torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        alpha=emo_alpha\n",
    "                    )\n",
    "\n",
    "                    if emo_vector is not None:\n",
    "                        emovec = emovec_mat + (1 - torch.sum(weight_vector)) * emovec\n",
    "                        # emovec = emovec_mat\n",
    "\n",
    "                    codes, speech_conditioning_latent = await self.gpt.inference_speech_vllm(\n",
    "                        spk_cond_emb,\n",
    "                        text_tokens,\n",
    "                        emo_cond_emb,\n",
    "                        cond_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        emo_cond_lengths=torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        emo_vec=emovec,\n",
    "                        num_return_sequences=autoregressive_batch_size\n",
    "                    )\n",
    "                codes = torch.tensor(codes, dtype=torch.long, device=self.device)\n",
    "                # print(f\"shape of codes: {codes[0][0:10]}, {codes.shape}\")\n",
    "                print(f\"shape of speech_conditioning_latent: {speech_conditioning_latent.shape}\")\n",
    "                gpt_gen_time += time.perf_counter() - m_start_time\n",
    "                if not has_warned and (codes[:, -1] != self.stop_mel_token).any():\n",
    "                    warnings.warn(\n",
    "                        f\"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). \"\n",
    "                        f\"Input text tokens: {text_tokens.shape[1]}. \"\n",
    "                        f\"Consider reducing `max_text_tokens_per_segment`({max_text_tokens_per_segment}) or increasing `max_mel_tokens`.\",\n",
    "                        category=RuntimeWarning\n",
    "                    )\n",
    "                    has_warned = True\n",
    "\n",
    "                code_lens = torch.tensor([codes.shape[-1]], device=codes.device, dtype=codes.dtype)\n",
    "                #                 if verbose:\n",
    "                #                     print(codes, type(codes))\n",
    "                #                     print(f\"codes shape: {codes.shape}, codes type: {codes.dtype}\")\n",
    "                #                     print(f\"code len: {code_lens}\")\n",
    "\n",
    "                code_lens = []\n",
    "                for code in codes:\n",
    "                    if self.stop_mel_token not in code:\n",
    "                        code_lens.append(len(code))\n",
    "                        code_len = len(code)\n",
    "                    else:\n",
    "                        len_ = (code == self.stop_mel_token).nonzero(as_tuple=False)[0] + 1\n",
    "                        code_len = len_ - 1\n",
    "                    code_lens.append(code_len)\n",
    "                codes = codes[:, :code_len]\n",
    "                code_lens = torch.LongTensor(code_lens)\n",
    "                code_lens = code_lens.to(self.device)\n",
    "                if verbose:\n",
    "                    print(codes, type(codes))\n",
    "                    print(f\"fix codes shape: {codes.shape}, codes type: {codes.dtype}\")\n",
    "                    print(f\"code len: {code_lens}\")\n",
    "\n",
    "                m_start_time = time.perf_counter()\n",
    "                use_speed = torch.zeros(spk_cond_emb.size(0)).to(spk_cond_emb.device).long()\n",
    "                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):\n",
    "                    latent = self.gpt(\n",
    "                        speech_conditioning_latent,\n",
    "                        text_tokens,\n",
    "                        torch.tensor([text_tokens.shape[-1]], device=text_tokens.device),\n",
    "                        codes,\n",
    "                        torch.tensor([codes.shape[-1]], device=text_tokens.device),\n",
    "                        emo_cond_emb,\n",
    "                        cond_mel_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        emo_cond_mel_lengths=torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),\n",
    "                        emo_vec=emovec,\n",
    "                        use_speed=use_speed,\n",
    "                    )\n",
    "                    # print(\"gpt latent\",latent.shape, latent[0][0:10])\n",
    "                    gpt_forward_time += time.perf_counter() - m_start_time\n",
    "\n",
    "                dtype = None\n",
    "                # with torch.amp.autocast(text_tokens.device.type, enabled=dtype is not None, dtype=dtype):\n",
    "                print(text_tokens.device.type)\n",
    "                m_start_time = time.perf_counter()\n",
    "                diffusion_steps = 13\n",
    "                inference_cfg_rate = 0.7\n",
    "                latent = self.s2mel.models['gpt_layer'](latent)\n",
    "                S_infer = self.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))\n",
    "                S_infer = S_infer.transpose(1, 2)\n",
    "                S_infer = S_infer + latent\n",
    "                target_lengths = (code_lens * 1.72).long()\n",
    "                # print(\"S_infer\",S_infer.shape, S_infer[0][0:10])\n",
    "                cond = self.s2mel.models['length_regulator'](S_infer,\n",
    "                                                             ylens=target_lengths,\n",
    "                                                             n_quantizers=3,\n",
    "                                                             f0=None)[0]\n",
    "                cat_condition = torch.cat([prompt_condition, cond], dim=1)\n",
    "                print(\"cat_condition\",cat_condition.shape, cat_condition[0][0:10])\n",
    "                print(\"ref_mel\",ref_mel.shape, ref_mel[0][0:10])\n",
    "                print(\"style\",style.shape, style[0][0:10])\n",
    "                vc_target = self.s2mel.models['cfm'].inference(cat_condition,\n",
    "                                                               torch.LongTensor([cat_condition.size(1)]).to(\n",
    "                                                                   cond.device),\n",
    "                                                               ref_mel, style, None, diffusion_steps,\n",
    "                                                               inference_cfg_rate=inference_cfg_rate)\n",
    "                vc_target = vc_target[:, :, ref_mel.size(-1):]\n",
    "                print(\"vc_target\",vc_target.shape, vc_target[0][0:10])\n",
    "                s2mel_time += time.perf_counter() - m_start_time\n",
    "\n",
    "                m_start_time = time.perf_counter()\n",
    "                wav = self.bigvgan(vc_target.float()).squeeze().unsqueeze(0)\n",
    "                print(wav.shape, wav[0][0:10])\n",
    "                bigvgan_time += time.perf_counter() - m_start_time\n",
    "                wav = wav.squeeze(1)\n",
    "\n",
    "                wav = torch.clamp(32767 * wav, -32767.0, 32767.0)\n",
    "                if verbose:\n",
    "                    print(f\"wav shape: {wav.shape}\", \"min:\", wav.min(), \"max:\", wav.max())\n",
    "                # wavs.append(wav[:, :-512])\n",
    "                wavs.append(wav.cpu())  # to cpu before saving\n",
    "        end_time = time.perf_counter()\n",
    "        self._set_gr_progress(0.9, \"save audio...\")\n",
    "        if interval_silence > 0:\n",
    "            wavs = self.insert_interval_silence(wavs, sampling_rate=sampling_rate, interval_silence=interval_silence)\n",
    "        # wav = torch.cat(wavs, dim=1)\n",
    "        crossfade_ms = 150\n",
    "        wav = self.crossfade_torch(wavs, crossfade_duration_ms=crossfade_ms, sampling_rate=sampling_rate)\n",
    "        wav_length = wav.shape[-1] / sampling_rate\n",
    "        print(f\">> gpt_gen_time: {gpt_gen_time:.2f} seconds\")\n",
    "        print(f\">> gpt_forward_time: {gpt_forward_time:.2f} seconds\")\n",
    "        print(f\">> s2mel_time: {s2mel_time:.2f} seconds\")\n",
    "        print(f\">> bigvgan_time: {bigvgan_time:.2f} seconds\")\n",
    "        print(f\">> Total inference time: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\">> Generated audio length: {wav_length:.2f} seconds\")\n",
    "        print(f\">> RTF: {(end_time - start_time) / wav_length:.4f}\")\n",
    "\n",
    "        # save audio\n",
    "        wav = wav.cpu()  # to cpu\n",
    "        if output_path:\n",
    "            # 直接保存音频到指定路径中\n",
    "            if os.path.isfile(output_path):\n",
    "                os.remove(output_path)\n",
    "                print(\">> remove old wav file:\", output_path)\n",
    "            if os.path.dirname(output_path) != \"\":\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            torchaudio.save(output_path, wav.type(torch.int16), sampling_rate)\n",
    "            print(\">> wav file saved to:\", output_path)\n",
    "            return output_path\n",
    "        else:\n",
    "            # 返回以符合Gradio的格式要求\n",
    "            wav_data = wav.type(torch.int16)\n",
    "            wav_data = wav_data.numpy().T\n",
    "            return (sampling_rate, wav_data)\n",
    "# index_tts.infer_parallel = infer_parallel\n",
    "import types\n",
    "index_tts.infer = types.MethodType(infer, index_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b47c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await index_tts.infer(spk_audio_prompt='data/tina_test.wav', text=\"\"\"Mike is showing great potential in his Fall In-Person Chess Lessons.\"\"\", output_path=\"test_parallel.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0)\n",
    "\n",
    "# [1293, 5823, 7684, 1627, 4425, 3492, 7088, 5129, 6199, 5200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddf0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting parallel inference...\n",
      "使用指定的情感向量\n",
      "INFO 09-16 14:59:27 [async_llm_engine.py:211] Added request req-93ee00a6-09a5-4ca0-9e73-ff6db7f16fa0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> GPT generation time (parallel): 1.26 seconds\n",
      ">> GPT forward pass time: 0.04 seconds\n",
      ">> S2MEL time: 0.39 seconds\n",
      ">> BigVGAN time: 0.06 seconds\n",
      ">> Total inference time: 1.89 seconds\n",
      ">> Generated audio length: 8.58 seconds\n",
      ">> Real-Time Factor (RTF): 0.2208\n",
      ">> WAV file saved to: test_parallel2.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_parallel2.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await index_tts.infer_parallel(spk_audio_prompt='data/tina_test.wav', text=\"\"\"Richoo takes care of the busy work. So you can focus on coaching kids, giving personalized guidance, and engaging families.\"\"\", output_path=\"test_parallel2.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
