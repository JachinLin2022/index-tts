{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef59f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> All random seeds have been set to 42 for reproducibility.\n",
      "INFO 09-17 09:55:49 [config.py:3131] Downcasting torch.float32 to torch.float16.\n",
      "INFO 09-17 09:55:49 [config.py:793] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-17 09:55:49 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0) with config: model='checkpoints/vllm', speculative_config=None, tokenizer='checkpoints/vllm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1818, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=checkpoints/vllm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \n",
      "INFO 09-17 09:55:49 [model_runner.py:1170] Starting to load model checkpoints/vllm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8de9375b7384d5c9d1e73df3cee06cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 09:55:49 [default_loader.py:280] Loading weights took 0.54 seconds\n",
      "INFO 09-17 09:55:50 [model_runner.py:1202] Model loading took 0.9233 GiB and 0.562023 seconds\n",
      "INFO 09-17 09:55:51 [worker.py:291] Memory profiling takes 0.39 seconds\n",
      "INFO 09-17 09:55:51 [worker.py:291] the current vLLM instance can use total_gpu_memory (23.52GiB) x gpu_memory_utilization (0.25) = 5.88GiB\n",
      "INFO 09-17 09:55:51 [worker.py:291] model weights take 0.92GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.16GiB; the rest of the memory reserved for KV Cache is 4.80GiB.\n",
      "INFO 09-17 09:55:51 [executor_base.py:112] # cuda blocks: 2619, # CPU blocks: 2184\n",
      "INFO 09-17 09:55:51 [executor_base.py:117] Maximum concurrency for 1818 tokens per request: 23.05x\n",
      "INFO 09-17 09:55:53 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f950747a3487a9145107d431a67ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 09:56:17 [model_runner.py:1670] Graph capturing finished in 24 secs, took 0.20 GiB\n",
      "INFO 09-17 09:56:17 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 27.08 seconds\n",
      ">> GPT weights restored from: checkpoints/gpt.pth\n",
      ">> semantic_codec weights restored from: ./checkpoints/hf_cache/models--amphion--MaskGCT/snapshots/265c6cef07625665d0c28d2faafb1415562379dc/semantic_codec/model.safetensors\n",
      "cfm loaded\n",
      "length_regulator loaded\n",
      "gpt_layer loaded\n",
      ">> s2mel weights restored from: checkpoints/s2mel.pth\n",
      ">> campplus_model weights restored from: ./checkpoints/hf_cache/models--funasr--campplus/snapshots/fb71fe990cbf6031ae6987a2d76fe64f94377b7e/campplus_cn_common.bin\n",
      "Loading weights from nvidia/bigvgan_v2_22khz_80band_256x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 09:56:29,593 WETEXT INFO found existing fst: /root/RichooAgent/third_party/index-tts/indextts/utils/tagger_cache/zh_tn_tagger.fst\n",
      "2025-09-17 09:56:29,594 WETEXT INFO                     /root/RichooAgent/third_party/index-tts/indextts/utils/tagger_cache/zh_tn_verbalizer.fst\n",
      "2025-09-17 09:56:29,594 WETEXT INFO skip building fst for zh_normalizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      ">> bigvgan weights restored from: nvidia/bigvgan_v2_22khz_80band_256x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 09:56:29,787 WETEXT INFO found existing fst: /venv/main/lib/python3.12/site-packages/tn/en_tn_tagger.fst\n",
      "2025-09-17 09:56:29,787 WETEXT INFO                     /venv/main/lib/python3.12/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-09-17 09:56:29,787 WETEXT INFO skip building fst for en_normalizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> TextNormalizer loaded\n",
      ">> bpe model loaded from: checkpoints/bpe.model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "from indextts.infer_v2_vllm import IndexTTS2\n",
    "# from indextts.infer_v2 import IndexTTS2\n",
    "import asyncio\n",
    "index_tts = IndexTTS2(cfg_path=\"checkpoints/config.yaml\", model_dir=\"checkpoints\", use_cuda_kernel=False, gpu_memory_utilization=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ddf0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting parallel inference...\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-12de9c34-817f-4d2f-8ddf-684ffa4fc602.\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-2b84af0a-9d11-458f-a2c7-e301e1ec8bc1.\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-dab655c2-efa2-44ea-8344-4de1caf20a5a.\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-7e9177be-fd61-44b8-9956-7021d4a10ff4.\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-1c27dff2-1d7a-4a85-a0e6-c0bdec578a5c.\n",
      "INFO 09-17 10:00:16 [async_llm_engine.py:211] Added request req-7849e10d-f9d5-40e9-a2f1-eeac5239e005.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 102.43it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 101.97it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 93.46it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 103.70it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 99.86it/s] \n",
      "100%|██████████| 20/20 [00:00<00:00, 90.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> GPT generation time (parallel): 1.34 seconds\n",
      ">> GPT forward pass time: 0.05 seconds\n",
      ">> S2MEL time: 1.24 seconds\n",
      ">> BigVGAN time: 0.27 seconds\n",
      ">> Total inference time: 3.28 seconds\n",
      ">> Generated audio length: 44.95 seconds\n",
      ">> Real-Time Factor (RTF): 0.0731\n",
      ">> WAV file saved to: test_parallel.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_parallel.wav'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import types\n",
    "from pydub import AudioSegment\n",
    "from indextts import infer_v2_vllm\n",
    "importlib.reload(infer_v2_vllm)\n",
    "from indextts.infer_v2_vllm import IndexTTS2\n",
    "index_tts.infer_stream = types.MethodType(IndexTTS2.infer_stream, index_tts)\n",
    "index_tts.infer_parallel = types.MethodType(IndexTTS2.infer_parallel, index_tts)\n",
    "\n",
    "count = 0\n",
    "concat_segment = None\n",
    "from io import BytesIO\n",
    "\n",
    "# async for chunk in index_tts.infer_stream(spk_audio_prompt='data/tina_test.wav', text=\"\"\"Tom could benefit from more consistent practice to really solidify his understanding and skills in chess. This is a common area for growth, and with some focused effort, we'll see great progress. To help Tom get more accustomed to chess, we recommend establishing a fun, regular practice routine at home. Tom could try playing one or two quick games daily, either with family or on a platform like ChessKid. Solving a few chess puzzles each week through an app or workbook is also an excellent way to sharpen his mind. Aiming for around 2-4 hours of practice a week will build a strong foundation and keep the game enjoyable and engaging for him.\"\"\", output_path=\"test_parallel2.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0):\n",
    "\n",
    "\n",
    "#     with open(f\"test_stream_{count}.wav\", \"wb\") as f:\n",
    "#         f.write(chunk)\n",
    "#     if concat_segment is None:\n",
    "#         concat_segment = AudioSegment.from_file(BytesIO(chunk))\n",
    "#     else:\n",
    "#         concat_segment = concat_segment.append(AudioSegment.from_file(BytesIO(chunk)), crossfade=0)\n",
    "\n",
    "# concat_segment.export(\"test_stream.mp3\", format=\"mp3\")\n",
    "await index_tts.infer_parallel(spk_audio_prompt='data/test_real.wav', text=\"\"\"Tom could benefit from more consistent practice to really solidify his understanding and skills in chess. This is a common area for growth, and with some focused effort, we'll see great progress. To help Tom get more accustomed to chess, we recommend establishing a fun, regular practice routine at home. Tom could try playing one or two quick games daily, either with family or on a platform like ChessKid. Solving a few chess puzzles each week through an app or workbook is also an excellent way to sharpen his mind. Aiming for around 2-4 hours of practice a week will build a strong foundation and keep the game enjoyable and engaging for him.\"\"\", output_path=\"test_parallel.wav\", verbose=False, max_text_tokens_per_segment=120, interval_silence=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# 加载两个音频文件\n",
    "audio1 = AudioSegment.from_mp3(\"test_stream_1.wav\")\n",
    "audio2 = AudioSegment.from_mp3(\"test_stream_2.wav\")\n",
    "\n",
    "merged_chunk = audio1.append(audio2, crossfade=0)\n",
    "merged_chunk.export(\"merged_chunk.mp3\", format=\"mp3\")\n",
    "# def stream_with_crossfade(audio1, audio2, crossfade_ms=250):\n",
    "#     # Step 1: 播放 audio1 主体部分（除去最后 crossfade_ms）\n",
    "#     main_part = audio1[:-crossfade_ms]\n",
    "#     yield main_part.raw_data  # 或导出为临时文件/分块传输\n",
    "\n",
    "#     # Step 2: 交叉淡出淡入部分\n",
    "#     fade_out = audio1[-crossfade_ms:].fade_out(crossfade_ms)\n",
    "#     fade_in = audio2[:crossfade_ms].fade_in(crossfade_ms)\n",
    "#     crossfaded = fade_out.overlay(fade_in)  # 混合\n",
    "#     yield crossfaded.raw_data\n",
    "\n",
    "#     # Step 3: 播放 audio2 剩余部分\n",
    "#     remaining = audio2[crossfade_ms:]\n",
    "#     yield remaining.raw_data\n",
    "# crossfade_ms = 250\n",
    "# a1 = audio1[:-crossfade_ms]\n",
    "# a2 = audio2[crossfade_ms:]\n",
    "# fade_out = audio1[-crossfade_ms:].fade_out(crossfade_ms)\n",
    "# fade_in = audio2[:crossfade_ms].fade_in(crossfade_ms)\n",
    "# crossfaded = fade_out.overlay(fade_in)\n",
    "\n",
    "# merged_chunk = a1.append(crossfaded, crossfade=0)\n",
    "# merged_chunk = merged_chunk.append(a2, crossfade=0)\n",
    "# merged_chunk.export(\"merged_chunk.mp3\", format=\"mp3\")\n",
    "# with open(\"merged_chunk.mp3\", \"wb\") as f:\n",
    "    # f.write(a1 + crossfaded + a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a16fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to receive chunk: 1.27 seconds\n",
      "Time taken to receive chunk: 1.13 seconds\n",
      "Time taken to receive chunk: 1.32 seconds\n",
      "Time taken to receive chunk: 1.17 seconds\n",
      "Time taken to receive chunk: 1.25 seconds\n",
      "Time taken to receive chunk: 1.31 seconds\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import requests\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import time\n",
    "payload = {\n",
    "    \"prompt_audio_url\": \"https://beezstorageblob.blob.core.windows.net/emailblob/cloned_voice_0e3a2d2d-7a24-41d2-b1f5-c7af7f3ab284.wav\",\n",
    "    \"text\": \"Tom could benefit from more consistent practice to really solidify his understanding and skills in chess. This is a common area for growth, and with some focused effort, we'll see great progress. To help Tom get more accustomed to chess, we recommend establishing a fun, regular practice routine at home. Tom could try playing one or two quick games daily, either with family or on a platform like ChessKid. Solving a few chess puzzles each week through an app or workbook is also an excellent way to sharpen his mind. Aiming for around 2-4 hours of practice a week will build a strong foundation and keep the game enjoyable and engaging for him.\",\n",
    "}\n",
    "\n",
    "concat_segment = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    with httpx.stream(\"POST\", \"http://127.0.0.1:62343/generate-speech-stream/\", json=payload, follow_redirects=True) as r:\n",
    "        # r.raise_for_status() # Ensure the request was successful\n",
    "        start_time = time.time()\n",
    "        for chunk in r.iter_bytes():\n",
    "            end_time = time.time()\n",
    "            print(f\"Time taken to receive chunk: {end_time - start_time:.2f} seconds\")\n",
    "            start_time = end_time\n",
    "            if concat_segment is None:\n",
    "                concat_segment = AudioSegment.from_file(BytesIO(chunk))\n",
    "            else:\n",
    "                concat_segment = concat_segment.append(AudioSegment.from_file(BytesIO(chunk)), crossfade=0)\n",
    "    concat_segment.export(\"test_stream.mp3\", format=\"mp3\")\n",
    "except httpx.HTTPError as e:\n",
    "    print(f\"An HTTP error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb1d9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Tom could benefit from more consistent practice to really solidify his understanding and skills in chess. This is a common area for growth and with some focused effort, we'll see great progress. To help Tom get more accustomed to chess, we recommend establishing a fun, regular practice routine at home. Tom could try playing one or two quick games daily, either with family or on a platform like ChessKit. Solving a few chess puzzles each week through an app or workbook is also an excellent way to sharpen his mind. Aiming for around 24 hours of practice a week will build a strong foundation and keep the game enjoyable and engaging for him.\", 'segments': [{'start': 0.0, 'end': 29.44, 'text': \"Tom could benefit from more consistent practice to really solidify his understanding and skills in chess. This is a common area for growth and with some focused effort, we'll see great progress. To help Tom get more accustomed to chess, we recommend establishing a fun, regular practice routine at home. Tom could try playing one or two quick games daily, either with family or on a platform like ChessKit.\"}, {'start': 29.44, 'end': 44.71, 'text': 'Solving a few chess puzzles each week through an app or workbook is also an excellent way to sharpen his mind. Aiming for around 24 hours of practice a week will build a strong foundation and keep the game enjoyable and engaging for him.'}], 'language': 'en', 'language_probability': 1, 'processing_time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# transcribe base64\n",
    "import base64\n",
    "import requests\n",
    "# with open(\"test_stream.mp3\", \"rb\") as f:\n",
    "with open(\"test_stream.mp3\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "    base64_data = base64.b64encode(data).decode(\"utf-8\")\n",
    "payload = {\n",
    "    \"audio_data\": base64_data,\n",
    "    \"file_extension\": \"wav\",\n",
    "    \"language\": \"en\"\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:62343/transcribe_base64/\", json=payload)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
